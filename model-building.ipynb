{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5092089,"sourceType":"datasetVersion","datasetId":2956926}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:46:55.282876Z","iopub.execute_input":"2024-04-18T18:46:55.283263Z","iopub.status.idle":"2024-04-18T18:47:10.816209Z","shell.execute_reply.started":"2024-04-18T18:46:55.283228Z","shell.execute_reply":"2024-04-18T18:47:10.815183Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-18 18:46:57.529581: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-18 18:46:57.529729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-18 18:46:57.681057: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define dataset directory\ndataset_dir = '/kaggle/input/real-life-violence/Real life violence dataset'\n\n# Define image dimensions and batch size\nimg_width, img_height = 224, 224\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:47:10.818243Z","iopub.execute_input":"2024-04-18T18:47:10.819108Z","iopub.status.idle":"2024-04-18T18:47:10.825089Z","shell.execute_reply.started":"2024-04-18T18:47:10.819073Z","shell.execute_reply":"2024-04-18T18:47:10.823184Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Create ImageDataGenerator for training data with validation split\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    validation_split=0.2  # Split data into 80% training and 20% validation\n)\n\n# Load training data from directory\ntrain_generator = train_datagen.flow_from_directory(\n    dataset_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='binary',  # Assuming binary classification (violent vs. non-violent)\n    subset='training'  # Specify training subset\n)\n\n# Load validation data from directory\nval_generator = train_datagen.flow_from_directory(\n    dataset_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='binary',  # Assuming binary classification (violent vs. non-violent)\n    subset='validation'  # Specify validation subset\n)\n\n# Check class indices\nprint(train_generator.class_indices)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:47:10.826335Z","iopub.execute_input":"2024-04-18T18:47:10.827881Z","iopub.status.idle":"2024-04-18T18:47:31.052554Z","shell.execute_reply.started":"2024-04-18T18:47:10.827848Z","shell.execute_reply":"2024-04-18T18:47:31.051568Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found 11245 images belonging to 2 classes.\nFound 2811 images belonging to 2 classes.\n{'nonviolence': 0, 'violence': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 2: Define CNN model architecture\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Step 3: Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:47:31.053680Z","iopub.execute_input":"2024-04-18T18:47:31.054637Z","iopub.status.idle":"2024-04-18T18:47:31.348631Z","shell.execute_reply.started":"2024-04-18T18:47:31.054606Z","shell.execute_reply":"2024-04-18T18:47:31.347429Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"}]},{"cell_type":"code","source":"epochs = 10\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // batch_size,\n    epochs=epochs,\n    validation_data=val_generator,\n    validation_steps=val_generator.samples // batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:47:31.350929Z","iopub.execute_input":"2024-04-18T18:47:31.351254Z","iopub.status.idle":"2024-04-18T19:57:30.740489Z","shell.execute_reply.started":"2024-04-18T18:47:31.351227Z","shell.execute_reply":"2024-04-18T19:57:30.739185Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m820s\u001b[0m 2s/step - accuracy: 0.5160 - loss: 0.6961 - val_accuracy: 0.6573 - val_loss: 0.5942\nEpoch 2/10\n\u001b[1m  1/351\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11:09\u001b[0m 2s/step - accuracy: 0.4688 - loss: 0.6108","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.4688 - loss: 0.3063 - val_accuracy: 0.8519 - val_loss: 0.2900\nEpoch 3/10\n\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m839s\u001b[0m 2s/step - accuracy: 0.6193 - loss: 0.6325 - val_accuracy: 0.6142 - val_loss: 0.6039\nEpoch 4/10\n\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5312 - loss: 0.3035 - val_accuracy: 0.3704 - val_loss: 0.3990\nEpoch 5/10\n\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m804s\u001b[0m 2s/step - accuracy: 0.7208 - loss: 0.5438 - val_accuracy: 0.7486 - val_loss: 0.5503\nEpoch 6/10\n\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7188 - loss: 0.2607 - val_accuracy: 0.7037 - val_loss: 0.4199\nEpoch 7/10\n\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 2s/step - accuracy: 0.7857 - loss: 0.4531 - val_accuracy: 0.6774 - val_loss: 0.6566\nEpoch 8/10\n\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7188 - loss: 0.2506 - val_accuracy: 0.6667 - val_loss: 0.2361\nEpoch 9/10\n\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m862s\u001b[0m 2s/step - accuracy: 0.8348 - loss: 0.3734 - val_accuracy: 0.7478 - val_loss: 0.5228\nEpoch 10/10\n\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8438 - loss: 0.1857 - val_accuracy: 0.8148 - val_loss: 0.1543\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"./model.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:58:13.720964Z","iopub.execute_input":"2024-04-18T19:58:13.721372Z","iopub.status.idle":"2024-04-18T19:58:13.998187Z","shell.execute_reply.started":"2024-04-18T19:58:13.721339Z","shell.execute_reply":"2024-04-18T19:58:13.996672Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}